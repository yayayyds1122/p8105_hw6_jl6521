---
title: "p8105_hw6_jl6521"
author: "Jiayi"
date: "2024-11-27"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(broom)
library(readr)
library(stringr)
library(purrr)
library(ggplot2)
library(modelr)
library(mgcv)
library(SemiPar)
set.seed(1)
```




## Problem 2
1. data cleaning
```{r problem2- data cleaning}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data = read_csv(url) %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city,", ",state),
    solved_status = ifelse(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)) %>% 
  filter(
    !city_state %in% c("Dallas, TX","Phoenix, AZ", "Kansas City, MO","Tulsa, AL"),
    victim_race %in% c("White", "Black")
    ) 
```

2. logistic regression for Baltimore, MD
```{r}
baltimore_df = 
  homicide_data %>%  
  filter(city_state == "Baltimore, MD") 
  
fit_logistic = 
  glm(solved_status ~ victim_age + victim_race + victim_sex, data = baltimore_df, family = binomial()) %>% 
  broom::tidy(conf.int = TRUE, conf.level = 0.95)

odds_ratio = fit_logistic %>%
  mutate(
    OR = exp(estimate),
    CI_low = exp(conf.low),
    CI_high = exp(conf.high)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(OR, CI_low, CI_high, p.value) %>%  
  knitr::kable(digits = 3)
```
3. glm for each of the cities 
```{r}
logistic_results <- homicide_data %>%
  group_by(city_state) %>%
  nest() %>%
  mutate(
    fit = map(data, ~ glm(solved_status ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
    results = map(fit, ~ broom::tidy(.x, conf.int = TRUE, conf.level = 0.95) %>%
      filter(term == "victim_sexMale") %>%
        mutate(
          OR = exp(estimate),
          CI_low = exp(conf.low),
          CI_high = exp(conf.high)
        ) %>% 
      select(OR, CI_low, CI_high, p.value))
  ) %>%
  unnest(results) %>% 
  select(OR, CI_low, CI_high, p.value)

logistic_results%>%
  knitr::kable(digits = 3)
  
```
4. Plot that shows the estimated ORs and CIs for each city
```{r}
logistic_results %>% 
  ggplot(aes(x=reorder(city_state, OR), y = OR))+
  geom_point()+
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high))+
  labs(
    title = "Estimated ORs and CIs for each city", 
    x = "City", 
    y = "OR"
  )+
  theme_minimal()+
  theme(axis.text.x=element_text(angle = 90, hjust =1))
```
Comment: The plot shows the odds ratio and confidence interval of solving homicides comparing male victims to female victims. From the plot we can see that CI for many cities crosses 1, showing that there are no differences between male and female victims. Albuquerque, NM has the highest odds ratio of male victims vs female victim and New York, NY has the lowest odds ratio.

## Problem 3
1. data cleaning
```{r}
birthweight = 
  read_csv("./data/birthweight.csv", show_col_types = FALSE) %>%
  janitor::clean_names() %>%
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8), labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
  parity = as.integer(parity),
    pnumlbw = as.integer(pnumlbw),
    pnumsga = as.integer(pnumsga))

colSums(is.na(birthweight))
```
Comment: I cleaned the variable names, and convert some categorical variables from numeric to factor and ensure some variables be integers. I also label those levels into real meaning rather than numbers. There are no missing values. 

2. Regression model for birth weight
```{r}
regression_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight)

summary(regression_model)

model_1 = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + parity + smoken, data = birthweight)

fit_1 = model_1 
#%>% 
   broom::tidy() 
  
diagnostic_1 = birthweight %>%
  modelr::add_predictions(fit_1) %>% 
  modelr::add_residuals(fit_1)
```
Modeling process: I think all these variables may be related to baby's birth weight. Baby's physical conditions: either being a boy or a girl and head circumference may be associated with the weight. Mother's features such as weight, height, age at delivery, pre-pregnancy BMI and weight may affect the baby's weight. The socioeconomic status (family's monthly income) may influence the diet the mom can have, which affects baby's weight. Gestational age may be related to the time the mother gives birth, which could link to the baby's weight.There may be some associations between mother and father's race with baby's weight, determined by gene.  Presence of malformations may affect weight. The number of cigarettes smoked may affect the health of baby, which influences the weight. Therefore, I run linear regression with all the variables first.

Next, I found that baby's sex, baby's head circumference at birth, baby's length at birth, mother's weight at delivery, gestational age in weeks, mother's race, number of live births prior to this pregnancy, and average number of cigarettes smoked per day during pregnancy are significant. Therefore I put those in my final model. 

3. Plot of residuals against fitted values
```{r plot}
ggplot(data = diagnostic_1, aes(x = pred, y = resid))+
  geom_point()
```


4. Comparison of models 
```{r}
model_2 = lm(bwt ~ blength + gaweeks, data = birthweight)
model_3 = lm(bwt ~ bhead*blength*babysex, data = birthweight)

cv_df <- crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_df = 
  cv_df %>%  
  mutate(
    model_1  = map(train, \(df) lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + parity + smoken, data = df)),
    model_2  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    model_3  = map(train, \(df) lm(bwt ~ bhead*blength*babysex, data = df)),
    rmse_1 =  map2_dbl(model_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2 = map2_dbl(model_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(model_3, test, \(mod, df) rmse(model = mod, data = df)))
```

```{r}
library(tidyverse)
cv_df %>%  
  select(starts_with("rmse")) %>%  
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%  
  mutate(model = fct_inorder(model)) %>%  
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
Comment: The figure shows the RMSE for each of the three models based on 100 cross-validation iterations. The model I chose (model 1) has the lowest RMSE score, indicating a good predictive power, and a narrow distribution, showing a good stability.  Model 2, with factors of length at birth and gestational age, has the highest RMSE and wide variability, showing clearly the poorest performance. The model 3 ranks in the middle.  Therefore, Model 1 may be the best choice. 



